<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">
    <title>Stanford CS329A | Self-Improving AI Agents</title>
    <!-- bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
    <!-- Google fonts -->
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="style.css" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <style>
      .navbar {
          margin-bottom: 0; 
      }

      .sec {
          padding: 10px 0;  
          margin-bottom: 0;  
      }

      #intro {
          margin-top: 10px;  
      }

      .course-title {
          padding: 15px 0;
          background-color: #8C1515;  /* Stanford red */
          color: white;
          margin-bottom: 10px;  
      }

      .course-title h1 {
          margin: 0;
          font-size: 24px;
      }

      h2 {
          margin-top: 10px;
          margin-bottom: 10px;
      }
    </style>
  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <a class="navbar-brand brand" href="index.html">CS329A Self-Improving AI Agents</a>
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
        </div>

        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav">
            <li><a href="#intro">Overview</a></li>
            <li><a href="#people">Staff</a></li>
            <li><a href="#syllabus">Syllabus</a></li>
            <li><a href="#grading">Grading</a></li>
            <li><a href="#policies">Policies</a></li>
          </ul>
        </div>
      </div>
    </nav>
    

    <div class="container sec" id="intro">
        <h2>Course Overview</h2>
        <p>
        This course covers the latest techniques and applications of AI agents that can continuously 
        improve themselves through interaction with themselves and the environment. 
        Our goal is that the students gain deep insights on building AI agents that take actions and use tools to 
        solve interesting and challenging real-world problems.
        </p>
      </div>

      <div class="container sec" id="people">
        <div class="col">
          <h2>Course Staff</h2>
          <div class="instructor">
            <a href="http://azaliamirhoseini.com/">
              <div class="instructorphoto">
                <img src="images/azalia.jpeg">
              </div>
              <div>Azalia Mirhoseini</div>
            </a>
            <div>Instructor</div>
          </div>
          <div class="instructor">
            <a href="https://www.achowdhery.com/">
              <div class="instructorphoto">
                <img src="images/aakanksha.jpeg">
              </div>
              <div>Aakanksha Chowdhery</div>
            </a>
            <div>Instructor</div>
          </div>
          <div class="instructor">
            <a href="https://cs.stanford.edu/~merty">
              <div class="instructorphoto">
                <img src="images/mert.jpg">
              </div>
              <div>Mert Yuksekgonul</div>
            </a>
            <div>Course Assistant</div>
          </div>
        </div>
        <!-- Logistics -->
        <div class="sechighlight">
          <div class="container sec" id="logistics">
            <h2>Logistics</h2>
            <ul>
              <li>
                <b>Lectures:</b> are on Monday/Friday 1:30 PM - 2:50 PM Pacific
                Time. In-person lectures will start with the first
                lecture.
              </li>
            </ul>
          </div>
        </div>
        <div class="container sec" id="syllabus">
          <h2>Syllabus</h2>
          <div class="table-responsive">
              <table class="table table-bordered">
                  <thead>
                      <tr>
                          <th>Date</th>
                          <th>Lecture</th>
                          <th>Main Readings</th>
                          <th>Additional Readings</th>
                      </tr>
                  </thead>
                  <tbody>
                      <tr>
                          <td>Jan 6</td>
                          <td>Course overview, scaling laws, emerging behavior, agent examples</td>
                          <td></td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Jan 10</td>
                          <td>Test Time Scaling</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2407.21787">Large Language Monkeys: Scaling Inference Compute with Repeated Sampling (Brown et al. 2024)</a></li>
                                  <li><a href="https://www.arxiv.org/abs/2409.15254">Archon: An Architecture Search Framework for Inference-Time Techniques (Saad-Falcon et al. 2024) </a></li>
                                  <li><a href="https://arxiv.org/abs/2408.03314">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters (Snell et al. 2024)</a></li>
                              </ul>
                          </td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2408.00724">Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models (Wu et al. 2024)</a></li>
                              </ul>
                          </td>
                      </tr>
                      <tr>
                          <td>Jan 13</td>
                          <td>Self-improvement Techniques: Verifiers</td>
                          <td>
                              <ul class="reading-list">
                                <li><a href="https://arxiv.org/abs/2110.14168">Training Verifiers to Solve Math Word Problems (Cobbe et al. 2021) </a></li>
                                <li><a href="https://arxiv.org/abs/2305.20050">Let's Verify step by step (Lightman et al. 2023)</a></li>
                                <li><a href="https://arxiv.org/abs/2211.14275">Solving math word problems with process- and outcome-based feedback (Uesato et al. 2022)</a></li>
                            </ul>
                          </td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2312.08935">Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations (Wang et al. 2023)</a></li>
                              </ul>
                          </td>
                      </tr>
                      <tr>
                          <td>Jan 17</td>
                          <td>Self-improvement Techniques with RL</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2203.14465">STaR: Bootstrapping Reasoning With Reasoning (Zelikman et al. 2022)</a></li>
                                  <li><a href="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback (Bai et al. 2022)</a></li>
                              </ul>
                          </td>
                          <td>
                              <ul class="reading-list">
                                <li><a href="https://arxiv.org/abs/2312.06585">Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models (Singh et al. 2023) </a></li>
                                <li><a href="https://arxiv.org/abs/2112.09332">WebGPT: Browser-assisted question-answering with human feedback (Nakano et al. 2021)</a></li>
                                <li><a href="https://arxiv.org/abs/2308.08998">Reinforced Self-Training (ReST) for Language Modeling (Gulcehre et al. 2023)</a></li>
                                <li><a href="https://arxiv.org/abs/2403.09629">Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking (Zelikman et al. 2024)</a></li>
                                <li><a href="https://arxiv.org/abs/2410.01679">VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment (Kazemnejad et al. 2024)</a></li>
                              </ul>
                          </td>
                      </tr>
                      <tr>
                          <td>Jan 20</td>
                          <td>MLK day - No Class</td>
                          <td></td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Jan 24</td>
                          <td>Self-improvement with search</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/1705.08439">Thinking Fast and Slow with Deep Learning and Tree Search (Anthony et al. 2017)</a></li>
                                  <li><a href="https://arxiv.org/abs/1712.01815">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (Silver et al. 2017)</a></li>
                                  <li><a href="https://www.science.org/doi/10.1126/science.abq1158">Competition-level code generation with AlphaCode (Li et al. 2022)</a></li>
                              </ul>
                          </td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Jan 27</td>
                          <td>AI research assistant</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2408.06292">The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery (Lu et al. 2024)</a></li>
                              </ul>
                          </td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Jan 31</td>
                          <td>Unifying Reasoning and Tool use/Actions in LLMs</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models (Yao et al. 2022)</a></li>
                                  <li><a href="https://arxiv.org/abs/2302.04761">Toolformer: Language Models Can Teach Themselves to Use Tools (Schick et al. 2023)</a></li>
                                  <li><a href="https://arxiv.org/abs/2305.15334">Gorilla: Large Language Model Connected with Massive APIs (Patil et al. 2023)</a></li>
                              </ul>
                          </td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2410.02089">RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning (Gehring et al. 2024)</a></li>
                              </ul>
                          </td>
                      </tr>
                      <tr>
                          <td>Feb 3</td>
                          <td>Reasoning/Actions across modalities (Web agents, Android Agents)</td>
                          <td></td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Feb 7</td>
                          <td>Eval benchmarks: capabilities</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://openai.com/index/introducing-swe-bench-verified/">SWE-bench Verified (OpenAI, 2024)</a></li>
                                  <li><a href="https://arxiv.org/abs/2410.07095">MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering (Chan et al. 2024)</a></li>
                                  <li><a href="https://arxiv.org/abs/2401.13649">VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks (Koh et al., 2024)</a></li>
                              </ul>
                          </td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2408.08926">Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models (Zhang et al., 2024)</a></li>
                                  <li><a href="https://arxiv.org/abs/2410.03859">SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domainss? (Yang et al., 2024)</a></li>
                                  <li><a href="https://arxiv.org/abs/2411.15114">RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts (Wijk et al., 2024)</a></li>
                              </ul>
                          </td>
                      </tr>
                      <tr>
                          <td>Feb 10</td>
                          <td>Planning and Multi-Step Reasoning 1/2</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2406.13094">Exploring and Benchmarking the Planning Capabilities of Large Language Models (Bohnet et al., 2024)</a></li>
                                  <li><a href="https://arxiv.org/abs/2310.04406">Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models (Zhou et al. 2023)</a></li>
                                  <li><a href="https://arxiv.org/abs/2309.17179">Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training (Feng et al. 2023)</a></li>
                              </ul>
                          </td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2310.04363">Amortizing intractable inference in large language models (Hu et al., 2023)</a></li>
                              </ul>
                          </td>
                      </tr>
                      <tr>
                          <td>Feb 14</td>
                          <td>AI coding agents</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://replit.com/">Replit</a></li>
                                  <li><a href="https://arxiv.org/abs/2405.15793">SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering (Yang et al. 2024)</a></li>
                                  <li><a href="https://arxiv.org/abs/2410.20285">SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement (Antoniades et al. 2024)</a></li>
                              </ul>
                          </td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Feb 17</td>
                          <td>President's day - No Class</td>
                          <td></td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Feb 21</td>
                          <td>Midterm Progress Presentations</td>
                          <td></td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Feb 24</td>
                          <td>Retrieval augmentation/Long Context</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2406.13121">Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More? (Lee et al. 2024)</a></li>
                              </ul>
                          </td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2209.11755">Promptagator: Few-shot dense retrieval from 8 examples (Dai et al., 2022)</a></li>
                                  <li><a href="https://www.anthropic.com/news/contextual-retrieval">Contextual Retrieval (Anthropic, 2024)</a></li>
                                  <li><a href="https://arxiv.org/abs/2405.16755">CHESS: Contextual Harnessing for Efficient SQL Synthesis (Talaei et al., 2024)</a></li>
                                  <li><a href="https://arxiv.org/abs/2407.11418">Semantic Operators: A Declarative Model for Rich, AI-based Analytics Over Text Data (Patel et al., 2024)</a></li>
                              </ul>
                          </td>
                      </tr>
                      <tr>
                          <td>Feb 28</td>
                          <td>Orchestration tools: AutoGen</td>
                          <td></td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Mar 3</td>
                          <td>Memory; State Management; Planning</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2404.03683">Stream of Search (SoS): Learning to Search in Language (Gandhi et al. 2024)</a></li>
                                  <li><a href="https://arxiv.org/abs/2409.13373">LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench (Valmeekam et al. 2024)</a></li>
                              </ul>
                          </td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Mar 7</td>
                          <td>Multi-agent systems & Future research areas</td>
                          <td></td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Mar 10</td>
                          <td>TBD (Security/Reliability)</td>
                          <td>
                              <ul class="reading-list">
                                  <li><a href="https://arxiv.org/abs/2406.13352">AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents (Debenedetti et al. 2024)</a></li>
                              </ul>
                          </td>
                          <td></td>
                      </tr>
                      <tr>
                          <td>Mar 14</td>
                          <td>TBD</td>
                          <td></td>
                          <td></td>
                      </tr>
                  </tbody>
              </table>
          </div>
        </div>
      </div>
    </div>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    
    <div class="container sec" id="grading">
      <h2>Grading</h2>
      <ul>
        <li><b>In-class paper presentation (15%)</b></li>
        <li><b>Weekly paper discussion questions (5%)</b>
          <ul><li>Due 10pm the night before class</li></ul>
        </li>
        <li><b>Project Proposal (10%)</b>
          <ul><li>Due January 24 at 9pm</li></ul>
        </li>
        <li><b>Project Milestone (20%)</b>
          <ul>
            <li>Due February 21 at 9pm</li>
            <li>5-minute presentation (5%)</li>
            <li>Report (15%)</li>
          </ul>
        </li>
        <li><b>Final Report Writeup (40%)</b>
          <ul>
            <li>Due March 17 at 9pm (late days cannot be used)</li>
            <li>Write up (30%)</li>
            <li>Artifact (10%) - code, model, evaluation dataset</li>
          </ul>
        </li>
        <li><b>Final Poster Presentation (10%)</b>
          <ul><li>March 19 (3-6pm)</li></ul>
        </li>
      </ul>
    </div>
    
    
    <div class="container sec" id="student-lectures">
      <h2>Student Lectures</h2>
      <p>Most weeks there will be at least one Student Lecture. Each Student Lecture will consist of a team of 2 to 3 students 
      that are responsible for reading all of the suggested reading and presenting a graded survey presentation (~45 Minutes).</p>
      <p>Format: 45 minute presentations followed by 15-20 minute Q&A sessions.</p>
    </div>

    <div class="container sec" id="research-projects">
      <h2>Research Projects</h2>
      <p>As a graduate seminar, research is a big part of the class. Students will work in teams of 2 to 3 to complete original 
      research. These projects should be broadly around agentic AI. Students will receive API credits to support their 
      development work.</p>

      <p>Your final report is due on <b>March 17, 9 p.m.</b>.</p>
    </div>

    <div class="container sec" id="policies">
      <h2>Course Policies</h2>
      
      <h3>Late Policy</h3>
      <ul>
        <li>All students have 4 free late days for the quarter.</li>
        <li>You may use up to 2 late days per assignment with no penalty.</li>
        <li>Late days can be used for assignments, project proposal, and project milestone.</li>
        <li>Late days cannot be used for the final project report.</li>
        <li>There will be a 25% penalty for each additional late day.</li>
        <li>No exceptions to this policy.</li>
      </ul>

      <h3>Audit Policy</h3>
      <p>Audits are not allowed for this course.</p>

      <h3>Communication with Course Staff</h3>
      <ul>
        <li>Please read the course documentation carefully before asking general questions.</li>
        <li>Questions should be asked during office hours or on EdStem.</li>
      </ul>
    </div>
  </body>
</html>
